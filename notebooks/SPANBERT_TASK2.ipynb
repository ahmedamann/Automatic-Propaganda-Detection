{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bdd001a-dc61-4f7a-92bd-919bc9e8d36c",
   "metadata": {},
   "source": [
    "## Prepare Data for Sequence Labelling Task (Propaganda Span Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee773ba-43dc-4032-a6d3-ca725707108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# prepare the data. Read tsv files into a dataframe and drop not_propaganda instances. \n",
    "def load_data_and_return_dataframe(data_dir=\"../data\", propaganda_only=False):\n",
    "    train_data_path = os.path.join(data_dir, \"propaganda_train.tsv\")\n",
    "    test_data_path = os.path.join(data_dir, \"propaganda_val.tsv\")\n",
    "    \n",
    "    train_data = pd.read_csv(train_data_path, delimiter=\"\\t\")\n",
    "    test_data = pd.read_csv(test_data_path, delimiter=\"\\t\")\n",
    "    \n",
    "    if propaganda_only:\n",
    "        train_data = train_data[train_data['label'] != 'not_propaganda'].reset_index(drop=True)\n",
    "        test_data = test_data[test_data['label'] != 'not_propaganda'].reset_index(drop=True)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_data_and_return_dataframe(propaganda_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21c0b02-711f-4cd5-b935-acf6ea0a9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_for_span_detection(sentence, label):\n",
    "    \"\"\"\n",
    "    Convert sentence into tokens and BIO labels for classification\n",
    "    \"\"\"\n",
    "    # remove BOS and EOS tags and tokenize\n",
    "    tokens = sentence.replace('<BOS>', ' <BOS> ').replace('<EOS>', ' <EOS> ').split()\n",
    "\n",
    "    span_start = tokens.index('<BOS>')\n",
    "    span_end = tokens.index('<EOS>')\n",
    "\n",
    "    # Remove the markers\n",
    "    tokens = [t for t in tokens if t not in ('<BOS>', '<EOS>')]\n",
    "    bio_labels = ['O'] * len(tokens)\n",
    "    bio_labels[span_start] = 'B-PROP'\n",
    "    \n",
    "    for i in range(span_start + 1, span_end - 1):\n",
    "        bio_labels[i] = 'I-PROP'\n",
    "\n",
    "    return tokens, bio_labels\n",
    "\n",
    "def process_data_for_bio(dataframe, func):\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label, sentence in dataframe.values:\n",
    "        tokens, bio_tags = func(sentence, label)\n",
    "        all_tokens.append(tokens)\n",
    "        all_labels.append(bio_tags)\n",
    "\n",
    "    return all_tokens, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b81241-0562-41fc-9628-68c4139433dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Apply to train/test\n",
    "tokens_train, tags_train = process_data_for_bio(train_data, convert_to_bio_for_span_detection)\n",
    "tokens_test, tags_test = process_data_for_bio(test_data, convert_to_bio_for_span_detection)\n",
    "\n",
    "# Make label2id map\n",
    "all_labels_set = sorted(set(tag for seq in tags_train for tag in seq))\n",
    "label2id = {label: i for i, label in enumerate(all_labels_set)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"tokens\": tokens_train,\n",
    "    \"labels\": [[label2id[t] for t in seq] for seq in tags_train]\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"tokens\": tokens_test,\n",
    "    \"labels\": [[label2id[t] for t in seq] for seq in tags_test]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9afcaedd-1cca-4c66-a43a-4be284af965a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"SpanBERT/spanbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e0add9-8360-46ab-b745-5b71f38ca1d1",
   "metadata": {},
   "source": [
    "## Tokenize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c4279-f6d9-403d-a735-26d2e7c747af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label_seq in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_seq[word_idx])\n",
    "            else:\n",
    "                # for the subword, convert B to I or repeat label\n",
    "                label_name = id2label[label_seq[word_idx]]\n",
    "                if label_name.startswith(\"B-\"):\n",
    "                    label_name = \"I-\" + label_name[2:]\n",
    "                label_ids.append(label2id[label_name])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize\n",
    "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e46a3-9009-4cb1-85ab-faab5520601b",
   "metadata": {},
   "source": [
    "## Define Compute Metrics Function Using seqeval's F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5a366-a3a8-49a8-ba5f-0083f4fa7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=2)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    true_preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        pred_labels = []\n",
    "        true_labels_seq = []\n",
    "        for p_i, l_i in zip(pred_seq, label_seq):\n",
    "            if l_i != -100:\n",
    "                pred_labels.append(id2label[p_i])\n",
    "                true_labels_seq.append(id2label[l_i])\n",
    "        true_preds.append(pred_labels)\n",
    "        true_labels.append(true_labels_seq)\n",
    "\n",
    "    # Sequence-level (seqeval)\n",
    "    seq_f1 = f1_score(true_labels, true_preds)\n",
    "\n",
    "    return {\n",
    "        \"seq_f1\": seq_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abada21b-be70-4d6d-9897-7e464becba86",
   "metadata": {},
   "source": [
    "## Define Training Arguements and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924cd36-b6de-4e60-8e15-6d4533fbf24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=2e-05,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"seq_f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5849f78-d684-46ac-b487-a71c2c98e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# init data collator for appropriate padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21261060-208c-402f-85ee-9c9404274061",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99c0f7-c08f-41db-b7de-c5cd66edc24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ede2a-f371-4dd5-b172-fb4138f5e406",
   "metadata": {},
   "source": [
    "## Savel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8a34a-07e9-473c-9b9d-4f4cb38368d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"spanbert-for-propaganda-span-detection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
